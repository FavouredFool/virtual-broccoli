% !TEX root = Konzept.tex
\chapter{Interaktionskonzept}
%Es zeigt sowohl den inhaltlichen und strukturellen Aufbau einer Anwendung als auch dessen Funktionsumfang. Wichtige Inhalte sind dabei die Informations- und Navigationsstruktur, das Verhalten und die Reaktionsweise von Interaktionselementen und Fehlermeldungen sowie grundlegende Interaktionsprinzipien.
Genau wie in der realen Welt muss man sich in virtuellen Welten mit unterschiedlichen Interaktionsformen auseinandersetzen. Besonders für Videospiele in der virtuellen Realität kann man sich umso mehr mit diesem Thema beschäftigen, da die Sinne der Spieler hierbei deutlich intensiver angesprochen werden.
\section{Interaktionstypen}
Gemäß des definierten Ziels ist die Interaktion innerhalb von VR der Schwerpunkt einer Software. Um auf die einzelnen Interaktionstypen einzugehen, wurden diese im Projekt in drei Subgruppen aufgeteilt.

\subsection{Visuelle Interaktion}
Ein Nutzer interagiert immer visuell mit der Welt, solange die Sicht in der VR-Brille nicht deutlich eingeschränkt wird. Während Sicht in VR als selbstverständlich angesehen wird, kann sie genutzt werden, um die Erfahrung eines Nutzers deutlich zu verändern.\\
\noindent Durch Verbesserungen in den Bereichen Auflösung, Sichtweite und weitere Hardware-spezifische Eigenschaften einer VR-Brille, jedoch auch an softwaretechnischen Gegebenheiten wie beispiels-weise der Eigenart der Render-Pipeline oder Qualität von 3D-Modellen, wird versucht, die Erfahrung innerhalb einer virtuellen Welt möglichst realistisch darzustellen. Jedoch ist der Spielraum an Veränderungen jeglicher Form in einer virtuellen Welt deutlich weitreichender als in der Realität. Dies bietet ein großes Repertoire an visuellen Interaktionsmöglichkeiten. Im Bereich der Augmented-Reality (AR) wird geforscht, um die Flexibilität der virtuellen Welt in die echte Welt einzubinden. Die Interaktionsmöglichkeiten sind dabei deutlich geringer als in einer virtuellen Welt. Allerdings wurden in dieser Branche bereits sehr interessante und hilfreiche Features entwickelt.

\subsection{Interaktion mit der Umgebung}
Dieser Interaktionstyp basiert auf jeglichem Feedback, mit welchem die umliegende Welt auf direkte Aktionen eines Nutzers reagiert. Dies zeigt sich in subtilen Details wie Fußabdrücken, die ein Charakter im Schnee hinterlässt, oder in offensichtlichen Auswirkungen auf die virtuelle Umgebung, wie beispielsweise ein umfallender Baum, nachdem ein Charakter ihn gefällt hat. Hierbei ist wichtig, dass dieser Interaktionstyp nicht den Akt des Baumfällens begutachtet, sondern sich vielmehr auf die darauf folgende Reaktion der Spielwelt, nämlich dass der Baum umfällt, fokussiert. Die Interaktion ist nicht auf eine virtuelle Umgebung beschränkt, wird jedoch vom Gefühl verstärkt, selbst in dieser lebendigen Welt zu stehen, da man es selbst direkt mitbekommt, anstatt sie nur auf einem 2D Bildschirm zu sehen.
\newpage
\noindent
\subsection{Haptische Interaktion}
Hingegen zu den obigen Interaktionsformen, welche auch in virtuellen Welten üblich sind, findet man die haptische Interaktionsform in virtuellen Welten eher selten wieder. Mithilfe von sogenannten Haptic Gloves konnten wir uns Gedanken über diese Interaktionsform machen. Doch was genau beschreibt eine haptische Interaktion?\\
Diese Form der Interaktion spricht den Tastsinn an. Dadurch wirken virtuelle Gegenstände, Wände und weitere Objekte deutlich realer und die Immersion in die virtuelle Welt wird enorm verbessert. Wenn sich diese Welt nahezu realistisch anfühlt, können Nutzer schnell vergessen, dass sie sich im Moment nicht in der Realität befinden.\\
Das Gefühl der haptischen Interaktion kann mit verschiedenen Wegen hervorgerufen werden. Letztendlich sind alle Umsetzungen auf Stimulierungen der Haut zurückzuführen. Dabei werden die Finger als primäre Körperteile zur Interaktion am häufigsten angesprochen. Vibration, Druck, Elektrizität, Hitze und Kälte, aber auch das physische Festsetzen einzelner Finger kann genutzt werden, um die virtuelle Welt darzustellen. Diese Gefühle können weiterhin auf verschiedene Teile des Körpers übertragen werden. Selbst die Verringerung der Temperatur eines Raumes kann dabei helfen, die Immersion in eine virtuelle Eislandschaft zu verbessern. An diesem Beispiel zeigt sich deutlich, dass haptischen Erfahrungen nicht nur positiver Natur sein müssen, um die Realität darzustellen.

\section{Technische Voraussetzung}
Wie bereits in der Einleitung erwähnt, sollte sich im Rahmen dieses Projekt mit unterschiedlichen Interaktionsformen in virtuellen Welten befasst werden. Hierbei benötigt es verständlicherweise Hardware, die für eine solche Welt ausgelegt ist. Hierzu verwendeten wir neben einer Valve Index, zwei HTC VIVE Focus 3, welche uns im Laufe des Projekts zur Verfügung gestellt wurden. Doch um sich mit der oben erwähnten haptischen Interaktion beschäftigt beschäftigen und haptisches Feedback zu ermöglichen, verwendeten wir die \dq SenseGloves\dq. 

\subsection{SenseGloves und Tracker}
Diese Handschuhe bieten eine eigene Software an, um die Inversion virtueller Realitäten durch haptisches Feedback zu intensivieren. Die Software ist jedoch nur geringfügig mit dem XR Interaction Toolkit von Unity kompatibel, wodurch es im Laufe des Projekts zu zahlreiche Hindernissen und Problemen kam, welche in kommenden Abschnitten etwas genauer erläutert werden.\\ 
\noindent Da die SenseGloves nur ihre relative Position zu sich selbst bestimmen können, benötigte es Unterstützung von externen Trackern, welche die Position und Rotation der Handschuhe im Raum erkennen. Hierbei wurden die \dq HTC VIVE Tracker 3.0\dq, welche über ein Verbindungsstück auf die Handschuhe gesetzt wurden, als solche Tracker verwendet.

\newpage \noindent
\subsection{Unity}
Die Umsetzung des Projekts erfolgte in der Game Engine \dq Unity\dq. Dies ist eine der derzeit populärsten Game Engines auf dem Markt und stellt den Industrie-Standard für Spielentwicklung in virtueller Realität dar. Darüber hinaus bietet Unity eine offiziell unterstützte Erweiterung namens \dq XR Interaction Toolkit\dq an, welche viele grundlegende VR-Funktionalitäten in ein Unity Projekt integriert und somit den Entwicklungsprozess deutlich beschleunigt. Des Weiteren unterstützt das Toolkit die externe Software \dq OpenXR\dq.

\subsection{Open XR}
OpenXR stellt eine allgemeine Schnittstelle für eine Vielzahl von VR-Brillen dar, indem es allgemeine Aktionen definiert, mit welchen das XR Interaction Toolkit und weitere Software arbeiten kann. Die device-basierten Mappings werden hierbei von OpenXR übernommen.

\subsection{Unity Input System}
In 2019 veröffentlichte Unity ein neues Input-System, welches vom Unity-eigenen XR Interaction Toolkit genutzt wurde. Dieses bietet eine Vielzahl an simplen Interaktionen in VR wie Bewegungs-typen, Greifen, Werfen und vielen weiteren. Da fundamentale Features nicht selbst entwickelt werden müssen, bildet es eine gute Grundlage für die Entwicklung in VR. Die Anbindung an das Input-System erfolgt im Einklang mit OpenXR. Dies setzt für alle unterstützten VR-Controller gemeinsame Aktionen fest und handhabt die gerätespezifische Ausführung selbst. Das Input-System muss sich somit ausschließlich Funktionalitäten wie \dq Select\dq oder \dq Trigger\dq anschalten, statt die Buttons der einzelnen Geräte anzusprechen. Das ist ein sehr simples und dynamisches System, welches es VR-Entwicklern leicht macht, Inputs aller unterstützten VR-Controller ohne signifikanten Extraaufwand zu verarbeiten.\\
\noindent Das System erfüllt zwar den gewollten Nutzen sehr gut, ist jedoch nicht gut zu erweitern. Wenn sich ein weiteres Gerät an die genannten Grundfunktionalitäten wie \dq Select\dq und \dq Trigger\dq an-bieten möchte, wird das System sehr schnell zu kompliziert. In einem Beispiel innerhalb des Projekts äußerte sich dies darin, dass die SenseGloves genutzt werden sollten, um mit einer speziellen Finger-Geste ein eingebautes Feature des XR Interaction Toolkits auszulösen, welches Teleportation ermöglicht. Während die Auswertung einer Geste durch die SenseGlove-eigene Softwarelösung trivial scheint, ist das Unity Input-System gegenüber externer Anbindungen so verschlossen, dass eine Anbindung an das XR Interaction Toolkit nicht möglich ist.\\
\noindent Dass es hierbei eine theoretisch mögliche Lösung für das Problem gibt, zeigt sich darin, dass ein Unity-Mitarbeiter hierzu ein Skript veröffentlicht hat, welches ein Input-System Profil für die HTC VIVE Tracker erstellt. Dieses bietet eine Funktionalität zum Abgreifen der realen Position und Rotation der Tracker an und half enorm dabei, die SenseGloves im Raum bewegbar zu machen. Jedoch ist das Skript für solch eine simple Hardware sehr komplex. Eine Anbindung der SenseGloves an das Input-System ist daher ein zu großes Unterfangen für das Projekt gewesen.\\
\noindent Somit sind VR-Grundfunktionen nur über VR-Controller ansprechbar. Anstatt zu versuchen, die gesamten Funktionalitäten neu zu schreiben, entschieden wir uns dafür, einem Spieler sowohl einen VR-Controller als auch einen SenseGlove an jeweils einer Hand nutzen zu lassen. Indem wir die benötigte Grundfunktionalität auf einen VR-Controller setzten, stand uns frei, die haptischen Handschuhe vorzuzeigen, ohne einen Durchlauf des Spiels unmöglich zu machen.

\section{Fortbewegung}
Fortbewegung und die damit verbundene Motion Sickness sind wichtige Themen in der VR-Entwicklung. Manche Probleme können hierbei durch verbesserte Hardware gelöst werden, indem beispielsweise die Bildschirmauflösung erhöht wird. Jedoch müssen viele sensible Ent-scheidungen in der Software selbst getroffen werden. Dabei ist die Art der Bewegung im Raum, welche auch Locomotion genannt wird, eine sehr grundlegende Richtungsentscheidung.

\subsection{Typen von Locomotion}
Locomotion teilt sich in zwei Subgruppen auf, Fortbewegung und Drehung. Dabei kann sich ein Spieler immer physisch im Raum bewegen und seinen Kopf drehen, um eine eins-zu-eins Übertragung dieser Bewegung in der virtuellen Welt zu erhalten. Ein Spieler kann sich somit ohne Knopf-Eingaben hinsetzen, umschauen und jegliche andere Aktivitäten vollführen, welche innerhalb seines Spielraums möglich sind.\\
Sollte das Spiel jedoch weitläufige Bewegungen fordern, wird dies schnell zu einem Problem. Ein Spieler sollte den Spielraum in der echten Welt natürlich nicht verlassen, da sich Spiele auf diesen Bereich begrenzen müssten, was sie zumeist nicht wollen. Spiele bieten einem Spieler somit die Option an, sich durch Knopf-Inputs fortzubewegen und zu drehen.
\subsubsection{Bewegung}
Fortbewegung kann \dq kontinuierlich\dq oder via \dq Teleportation\dq stattfinden. Ersteres nutzt einen Joystick, um den Spieler in eine Richtung zu bewegen. Diese Bewegung ist fortwährend in Echtzeit. Hingegen zum virtuellen Körper, bewegt sich der reale hierbei nicht. Viele Nutzer erfahren zumindest initial das Gefühl, dass der Körper unter ihnen weggezogen wird. Dies führt häufig zu Motion Sickness. Jedoch ist diese Art der Fortbewegung sehr schnell, präzise und somit in einem schnellen Spiel von Vorteil.\\
Teleportation kann vom Spieler über einen Knopfdruck aktiviert werden. Daraufhin zeigt die Software einen Strahl an, mit welchem eine Zielposition auf einem Untergrund bestimmt werden kann. Eine Positionsänderung zu diesem Ziel ist meist sehr schnell, doch da oftmals ein kurzes Fading genutzt wird, kann Motion Sickness für die meisten Nutzer vermieden werden. Diese Art der Fortbewegung ist hingegen langsamer und in manchen Spielen eher störend.

\subsubsection{Drehung}
Damit sich Spieler nicht in der echten Welt um sich selbst drehen müssen, kann Rotation auch per Knopfdruck erledigt werden. Erneut gibt es zwei Lösungen, \dq kontinuierlich\dq und \dq snapping\dq . Erstere stellt hierbei erneut eine konstante Drehung in Echtzeit mit einer festgelegten Geschwindigkeit dar, wenn ein Joystick in die jeweilige Richtung bewegt werden. Ebenso wie bei der Fortbewegung führt diese Form der Rotation oftmals zu Motion Sickness, da die Augen eine Drehung erfahren, die der Körper selbst nicht vollführt.\\
Hingegen dreht die Snapping-Option den Blick des Spielers in der virtuellen Welt auf Knopfdruck um einen festgelegten Winkel, was Motion Sickness in den meisten Fällen vorbeugt, da keine Bewegung simuliert wird, sondern die Blickrichtung einfach und direkt verändert wird.

